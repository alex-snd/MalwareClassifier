from os.path import join

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from matplotlib import cm
from torch import nn, optim
from torch.nn import functional as F
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader

from src.Common import config
from src.Common.Extraction import LabelExtractor
from src.Common.utils import (get_train_data_base_path, get_validation_data_base_path, create_training_space,
                              get_check_point_parameters_path, get_test_data_base_path, get_file_id)
from src.DB.Datasets import LabeledDataSet
from src.DL.utils import loss_visualization, save_parameters, array_to_image


class CVAE(nn.Module):
    def __init__(self, d_in: int, d_h: int, d_l: int, n_classes: int, dropout_rate: float = 0.1):
        super(CVAE, self).__init__()

        self.d_in = d_in
        self.d_h = d_h
        self.d_l = d_l
        self.n_classes = n_classes
        self.dropout_rate = dropout_rate

        if self.dropout_rate == 0:
            self._encode = nn.Sequential(
                nn.Linear(self.d_in + n_classes, self.d_h),
                nn.ReLU())

            self._decode = nn.Sequential(
                nn.Linear(self.d_l + n_classes, self.d_h),
                nn.ReLU(),
                nn.Linear(self.d_h, self.d_in),
                nn.Sigmoid())
        else:
            self._encode = nn.Sequential(
                nn.Linear(self.d_in + n_classes, self.d_h),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate))

            self._decode = nn.Sequential(
                nn.Linear(self.d_l + n_classes, self.d_h),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.d_h, self.d_in),
                nn.Sigmoid())

        self.to_mu = nn.Linear(self.d_h, self.d_l)
        self.to_log_var = nn.Linear(self.d_h, self.d_l)

    def _re_parameterize(self, mu, log_var) -> torch.tensor:
        if self.training:
            std = torch.exp(0.5 * log_var)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu

    def encode(self, x, c) -> tuple:
        x = torch.cat([x, c], 1)

        x = self._encode(x)
        mu = self.to_mu(x)
        log_var = self.to_log_var(x)
        z = self._re_parameterize(mu, log_var)

        return mu, log_var, z

    def decode(self, z, c) -> torch.tensor:
        z = torch.cat([z, c], 1)

        return self._decode(z)

    def forward(self, x, c) -> tuple:
        mu, log_var, z = self.encode(x, c)
        decoded = self.decode(z, c)

        return mu, log_var, decoded

    def save_parameters(self, filename: str):
        torch.save(self.state_dict(), filename)

    def load_parameters(self, filename: str, device: torch.device):
        self.load_state_dict(torch.load(filename, map_location=device))


def cvae_loss_function(dec_en_batch_data: torch.tensor, batch_data: torch.tensor, mu: torch.tensor,
                       log_var: torch.tensor) -> torch.tensor:
    kld = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    bce = F.binary_cross_entropy(dec_en_batch_data, batch_data, reduction='sum')

    return (bce + kld) / (batch_data.shape[0] * batch_data.shape[1])


def epoch_training(i_epoch: int, model: nn.Module, train_loader: DataLoader, optimizer, device: torch.device,
                   loss_function: eval, log_interval: int, training_space: dict, loss_history: dict,
                   printable: bool = True) -> None:
    model.train()

    train_loss = 0
    batch_idx = 1

    for batch_idx, (batch_data, batch_labels) in enumerate(train_loader, start=1):
        optimizer.zero_grad()

        batch_data = batch_data.type(torch.float32).to(device)
        batch_labels = batch_labels.type(torch.float32).to(device)

        mu, log_var, dec_en_batch_data = model(batch_data, batch_labels)

        loss = loss_function(dec_en_batch_data, batch_data, mu, log_var)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

        if batch_idx % log_interval == 0:
            i_epoch_info = f'Train Epoch: {i_epoch:>3} [{batch_idx * len(batch_data)}/{len(train_loader.dataset)} ' \
                           f'{100. * batch_idx / len(train_loader):.0f}%)] ' \
                           f'CVAE loss: {loss.item():>7.4f}'

            if printable:
                print(i_epoch_info)

            loss_history['loss'].append(loss.item())

            with open(training_space['log'], 'a') as logger:
                logger.write(i_epoch_info + '\n')

    train_loss /= batch_idx

    epoch_info = f'====> Epoch: {i_epoch} Average loss: {train_loss:.4f}'

    if printable:
        print(epoch_info)

    loss_history['avg_loss'].append(train_loss)

    with open(training_space['log'], 'a') as logger:
        logger.write(epoch_info + '\n')


def epoch_validation(model: nn.Module, validation_loader: DataLoader, device: torch.device, loss_function: eval,
                     training_space: dict, loss_history: dict, printable: bool = True) -> None:
    model.eval()
    avg_cvae_loss = 0
    batch_idx = 1

    with torch.no_grad():
        for batch_idx, (batch_data, batch_labels) in enumerate(validation_loader, start=1):
            batch_data = batch_data.type(torch.float32).to(device)
            batch_labels = batch_labels.type(torch.float32).to(device)

            mu, log_var, dec_en_batch_data = model(batch_data, batch_labels)

            avg_cvae_loss += loss_function(dec_en_batch_data, batch_data, mu, log_var)

    avg_cvae_loss /= batch_idx

    validation_info = f'====> Validation loss: {avg_cvae_loss:.4f}\n'

    loss_history['valid_loss'].append(avg_cvae_loss)

    if printable:
        print(validation_info)

    with open(training_space['log'], 'a') as logger:
        logger.write(validation_info + '\n')


def epoch_visualization(training_space: dict, loss_history: dict, model: nn.Module, i_epoch: int,
                        scatter_loader: DataLoader, performance_data_base: LabeledDataSet,
                        device: torch.device) -> None:
    loss_visualization(training_space, loss_history, i_epoch)
    cvae_performance(training_space, model, performance_data_base, i_epoch, device)

    if model.d_l == 2:
        cvae_scatter_latent_space(training_space, model, scatter_loader, i_epoch, device)


def cvae_performance(training_space: dict, model: nn.Module, performance_data_base: LabeledDataSet, i_epoch: int,
                     device: torch.device, n_to_show: int = 5) -> None:
    def save_performance(_image: np.array, _i: int, _i_epoch: int) -> None:
        plt.set_cmap('GnBu')
        plt.figure(figsize=(19.2, 10.8))
        plt.axis('off')
        plt.tight_layout()
        plt.imshow(_image)
        plt.savefig(join(training_space['performance'], f'{_i}_epoch_{_i_epoch}.png'), dpi='figure', format='png',
                    bbox_inches='tight')

        plt.clf()
        plt.close('all')

    model.eval()

    images = list()
    n = len(performance_data_base) if len(performance_data_base) < n_to_show else n_to_show
    width = 10

    for i, (i_sample, i_label) in enumerate(performance_data_base):
        if i == n_to_show:
            break

        i_sample = torch.tensor(i_sample, dtype=torch.float32).to(device)
        i_label = torch.tensor(i_label, dtype=torch.float32).to(device)

        i_sample = i_sample.view(-1, i_sample.shape[0])
        i_label = i_label.view(-1, i_label.shape[0])

        _, _, i_decoded = model(i_sample, i_label)

        i_sample = array_to_image(i_sample.cpu().numpy())
        i_decoded = array_to_image(i_decoded.detach().cpu().numpy())
        i_diff = np.abs(i_sample - i_decoded)

        im_size = i_sample.shape[0]

        image = np.zeros((im_size, 3 * im_size + 2 * width))

        image[:, :im_size] = i_sample
        image[:, im_size + width: 2 * im_size + width] = i_decoded
        image[:, 2 * im_size + 2 * width:] = i_diff

        images.append(image)

        save_performance(image, i, i_epoch)

    im_h = images[0].shape[0]
    im_w = images[0].shape[1]
    width = 5

    image = np.zeros((n * im_h + (n - 1) * width, im_w))

    for i in range(n):
        image[i * im_h + i * width: (i + 1) * im_h + i * width, :] = images[i]

    save_performance(image, n, i_epoch)


def cvae_scatter_latent_space(training_space: dict, model: nn.Module, scatter_loader: DataLoader, i_epoch: int,
                              device: torch.device) -> None:
    model.eval()

    n = len(scatter_loader.dataset)
    dots = np.zeros((n, 2))

    for i, (i_sample, i_label) in enumerate(scatter_loader):
        i_sample = i_sample.type(torch.float32).to(device)
        i_label = i_label.type(torch.float32).to(device)

        _, _, i_encoded = model.encode(i_sample, i_label)

        batch_size = i_encoded.shape[0]
        dots[i * batch_size: (i + 1) * batch_size, :] = i_encoded.detach().cpu().numpy()

    fig, ax = plt.subplots(figsize=(19.2, 10.8))
    ax.scatter(dots[:, 0], dots[:, 1], c='lightseagreen', marker='.')
    ax.set_title(f'Latent space')
    fig.tight_layout()
    plt.savefig(join(training_space['latent_space'], f'scatter_epoch_{i_epoch}.png'), dpi='figure', format='png',
                bbox_inches='tight')

    plt.clf()
    plt.close('all')


def cvae_scatter_labeled_latent_space(training_space: dict, model: nn.Module, scatter_loader: DataLoader,
                                      device: torch.device, n_to_show: int = 1000) -> None:
    def one_hot_to_index(_i_label: torch.tensor) -> list:
        return list(torch.argmax(_i_label, dim=1).numpy())

    model.eval()

    if n_to_show > len(scatter_loader.dataset):
        n_to_show = len(scatter_loader.dataset)

    dots = np.zeros((n_to_show, 2))
    labels = []

    for i, (i_sample, i_label) in enumerate(scatter_loader):
        if i == n_to_show:
            break

        i_sample = i_sample.type(torch.float32).to(device)
        i_label = i_label.type(torch.float32).to(device)

        _, _, i_encoded = model.encode(i_sample, i_label)

        batch_size = i_encoded.shape[0]
        dots[i * batch_size: (i + 1) * batch_size, :] = i_encoded.detach().cpu().numpy()
        labels.extend(one_hot_to_index(i_label))

    ticks_names = LabelExtractor.labels
    color_len = max(labels) / len(ticks_names)
    ticks = list(np.arange(color_len / 2, len(ticks_names) * color_len, color_len))

    color_map = cm.get_cmap('rainbow', len(ticks_names))

    fig, ax = plt.subplots(figsize=(19.2, 10.8))
    scatter = ax.scatter(dots[:, 0], dots[:, 1], c=labels, cmap=color_map, marker='o', alpha=1.0)
    color_bar = fig.colorbar(scatter, ticks=ticks, ax=ax, drawedges=True)
    color_bar.ax.set_yticklabels(ticks_names)
    ax.set_title(f'Latent space')
    fig.tight_layout()
    plt.savefig(join(training_space['latent_space'], f'labeled_scatter.png'), dpi='figure', format='png',
                bbox_inches='tight')

    plt.clf()
    plt.close('all')


def generate_data_base(model: nn.Module, device: torch.device, each_class: int = 5000,
                       batch_size: int = 512) -> None:
    def batch_decoding(_batch_size: int) -> None:
        batch_z = torch.randn((_batch_size, model.d_in), dtype=torch.float32, device=device)
        batch_labels = torch.stack([labels[label], ] * _batch_size).type(torch.float32).to(device)

        _, _, batch_data = model(batch_z, batch_labels)

        data.append(torch.cat([mark, batch_data.detach().cpu()], dim=1))

    torch.manual_seed(2531)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    last_batch_size = each_class % batch_size
    n_classes = 8
    labels = torch.eye(n_classes, n_classes)

    model.eval()

    training_space = create_training_space(model_name='CVAE', evaluate=True)
    model.load_parameters(get_check_point_parameters_path(training_space['check_point']))

    data = list()

    for label in range(n_classes):

        mark = torch.zeros((batch_size, 2)).fill_(label)
        for _ in range(each_class // batch_size):
            batch_decoding(batch_size)

        mark = torch.zeros((last_batch_size, 2)).fill_(label)
        if last_batch_size != 0:
            batch_decoding(last_batch_size)

    data = torch.cat(data, dim=0).numpy()
    data_base = pd.DataFrame(data)

    data_base_id = get_file_id(config.data_base_path)
    data_base.to_csv(path_or_buf=join(config.data_base_path, f'artificial_database_{data_base_id:_>3}.csv'),
                     index=False)


def training() -> None:
    torch.manual_seed(2531)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 128
    n_epochs = 120
    lr = 1e-2
    lr_step_interval = 105
    pre_trained = False

    log_interval = 3
    saving_interval = 10
    visualization_interval = 10

    train_data_base = get_train_data_base_path('train.csv')
    validation_data_base = get_validation_data_base_path('validation.csv')
    test_data_base = get_test_data_base_path('test.csv')

    train_data_set = LabeledDataSet(train_data_base, one_hot=True)
    validation_data_set = LabeledDataSet(validation_data_base, one_hot=True)
    test_data_set = LabeledDataSet(test_data_base, one_hot=True)

    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True, num_workers=1)
    validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False, num_workers=1)
    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False, num_workers=1)

    training_space = create_training_space(model_name='CVAE')

    n_classes = train_data_set.n_classes
    d_in = train_data_set.sample_size
    d_h = 100
    d_l = 2
    dropout_rate = 0.4

    model = CVAE(d_in=d_in, d_h=d_h, d_l=d_l, n_classes=n_classes, dropout_rate=dropout_rate).to(device)

    if pre_trained:
        model.load_parameters(get_check_point_parameters_path(training_space['check_point']), device=device)

    loss_function = cvae_loss_function
    optimizer = optim.Adam(model.parameters(), lr=lr)
    ae_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_step_interval)

    with open(training_space['log'], 'w') as logger:
        logger.write(train_data_base + '\n')
        logger.write(validation_data_base + '\n')
        logger.write(f'batch size: {batch_size}\n')
        logger.write(f'n_epochs: {n_epochs}\n')
        logger.write(f'lr: {lr}\n')
        logger.write(f'lr_step_interval: {lr_step_interval}\n')
        logger.write(f'pre_trained: {pre_trained}\n')
        logger.write(str(model))
        logger.write(f'\n{optimizer}\n')

    loss_history = {'loss': [], 'avg_loss': [], 'valid_loss': []}

    try:
        for i_epoch in range(1, n_epochs + 1):
            epoch_training(i_epoch, model, train_loader, optimizer, device, loss_function, log_interval,
                           training_space, loss_history)
            epoch_validation(model, validation_loader, device, loss_function, training_space, loss_history)

            if i_epoch % saving_interval == 0:
                save_parameters(model, training_space, i_epoch)

            if i_epoch % visualization_interval == 0:
                epoch_visualization(training_space, loss_history, model, i_epoch, test_loader,
                                    test_data_set, device)

            ae_lr_scheduler.step()

        if model.d_l == 2:
            cvae_scatter_labeled_latent_space(training_space, model, test_loader, device)

    except KeyboardInterrupt:
        print('Interrupted')


if __name__ == '__main__':
    matplotlib.rcParams.update({'font.size': 22, 'figure.facecolor': (239 / 255, 246 / 255, 231 / 255),
                                'axes.facecolor': (239 / 255, 246 / 255, 231 / 255),
                                'savefig.facecolor': (239 / 255, 246 / 255, 231 / 255)})

    training()

    pass
