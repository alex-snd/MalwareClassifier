import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader

from src.Common.utils import (get_train_data_base_path, get_validation_data_base_path, create_training_space,
                              get_check_point_parameters_path, get_test_data_base_path)
from src.DB.Datasets import LabeledDataSet
from src.DL.AE import AE
from src.DL.Classifier import (epoch_training, epoch_validation, epoch_visualization, testing, save_parameters)


class AEClassifier(nn.Module):
    def __init__(self, ae: AE, n_classes: int):
        super(AEClassifier, self).__init__()

        self.ae = ae
        self.ae.eval()

        for param in self.ae.parameters():
            param.requires_grad = False

        # classifier
        self.cl_fc1 = nn.Linear(self.ae.d_l, self.ae.d_l)
        self.cl_fc2 = nn.Linear(self.ae.d_l, n_classes)

    def forward(self, x):
        with torch.no_grad():
            z = self.ae.encode(x.view(-1, self.ae.d_in))

        x = F.relu(self.cl_fc1(z))

        return F.softmax(self.cl_fc2(x), dim=1)

    def save_parameters(self, filename: str):
        torch.save(self.state_dict(), filename)

    def load_parameters(self, filename: str, device: torch.device):
        self.load_state_dict(torch.load(filename, map_location=device))


def training() -> None:
    torch.manual_seed(2531)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 128
    n_epochs = 40
    lr = 1e-3
    lr_step_interval = 30
    log_interval = 3
    saving_interval = 10
    visualization_interval = 10
    pre_trained = False

    train_data_base = get_train_data_base_path('train.csv')
    validation_data_base = get_validation_data_base_path('validation.csv')
    test_data_base = get_test_data_base_path('test.csv')
    # remains_data_base = get_data_base_path('remains.csv')
    # artificial_data_base = get_data_base_path('artificial_database.csv')

    train_data_set = LabeledDataSet(train_data_base)
    validation_data_set = LabeledDataSet(validation_data_base)
    test_data_set = LabeledDataSet(test_data_base)
    # remains_data_set = LabeledDataSet(remains_data_base)
    # artificial_data_set = LabeledDataSet(artificial_data_base)

    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True,
                              num_workers=1, pin_memory=True)
    validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False,
                                   num_workers=1, pin_memory=True)
    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False,
                             num_workers=1, pin_memory=True)
    # remains_loader = DataLoader(remains_data_set, batch_size=batch_size, shuffle=False,
    #                             num_workers=1, pin_memory=True)
    # artificial_loader = DataLoader(artificial_data_set, batch_size=batch_size, shuffle=False,
    #                                num_workers=1, pin_memory=True)

    training_space = create_training_space(model_name='AEClassifier')

    d_in = train_data_set.sample_size
    d_l = 200
    n_classes = train_data_set.n_classes

    ae = AE(d_in=d_in, d_l=d_l).to(device)
    ae.load_parameters(get_check_point_parameters_path(training_space['check_point']), device=device)

    classifier = AEClassifier(ae=ae, n_classes=n_classes).to(device)

    if pre_trained:
        classifier.load_parameters(get_check_point_parameters_path(training_space['check_point']), device=device)

    optimizer = optim.Adam(classifier.parameters(), lr=lr)
    classifier_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_step_interval)

    loss_func = nn.CrossEntropyLoss()

    with open(training_space['log'], 'w') as logger:
        logger.write(train_data_base + '\n')
        logger.write(validation_data_base + '\n')
        logger.write(f'batch size: {batch_size}\n')
        logger.write(f'n_epochs: {n_epochs}\n')
        logger.write(f'lr: {lr}\n')
        logger.write(f'lr_step_interval: {lr_step_interval}\n')
        logger.write(f'pre_trained: {pre_trained}\n')
        logger.write(str(classifier))
        logger.write(f'{str(optimizer)}\n')
        logger.write(f'{str(loss_func)}\n')

    loss_history = {'loss': [], 'avg_loss': [], 'valid_loss': []}
    metrics_history = {'f1': [], 'valid_f1': [], 'valid_precision': [], 'valid_recall': []}

    try:
        for i_epoch in range(1, n_epochs + 1):
            epoch_training(i_epoch, classifier, train_loader, optimizer, device, log_interval, training_space,
                           loss_history, metrics_history, loss_func)
            epoch_validation(classifier, validation_loader, device, training_space, loss_history, metrics_history,
                             loss_func)

            if i_epoch % saving_interval == 0:
                save_parameters(classifier, training_space, i_epoch)

            if i_epoch % visualization_interval == 0:
                epoch_visualization(training_space, loss_history, metrics_history, i_epoch)

            classifier_lr_scheduler.step()

        testing(classifier, test_loader, device, training_space, loss_func)
        # testing(classifier, remains_loader, device, training_space, loss_func)
        # testing(classifier, artificial_loader, device, training_space, loss_func)

    except KeyboardInterrupt:
        print('Interrupted')


if __name__ == '__main__':
    training()

    pass
