import matplotlib
import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader

from src.Common.utils import (get_train_data_base_path, get_validation_data_base_path, create_training_space,
                              get_check_point_parameters_path, get_test_data_base_path)
from src.DB.Datasets import UnLabeledDataSet, LabeledDataSet
from src.DL.AE import (epoch_training, epoch_validation, epoch_visualization, ae_scatter_labeled_latent_space)
from src.DL.utils import save_parameters


class AE2(nn.Module):
    def __init__(self, d_in: int, d_h: int, d_l: int, dropout_rate: float):
        super(AE2, self).__init__()

        self.d_in = d_in
        self.d_l = d_l
        self.d_h = d_h
        self.dropout_rate = dropout_rate

        # encoder
        self.en_fc1 = nn.Linear(self.d_in, d_h)
        self.en_fc2 = nn.Linear(self.d_h, d_l)

        # decoder
        self.dec_fc1 = nn.Linear(d_l, d_h)
        self.dec_fc2 = nn.Linear(d_h, d_in)

    def __str__(self):
        str_repr = super(AE2, self).__str__()
        return str_repr + f'\nDropout rate: {self.dropout_rate}\n'

    def encode(self, x):
        x = F.relu(self.en_fc1(x))
        x = nn.Dropout(self.dropout_rate)(x)

        # return F.relu(self.en_fc2(x))
        return self.en_fc2(x)

    def decode(self, z):
        z = F.relu(self.dec_fc1(z))
        z = nn.Dropout(self.dropout_rate)(z)

        return torch.sigmoid(self.dec_fc2(z))

    def forward(self, x):
        z = self.encode(x.view(-1, self.d_in))

        return self.decode(z)

    def save_parameters(self, filename: str):
        torch.save(self.state_dict(), filename)

    def load_parameters(self, filename: str, device: torch.device):
        self.load_state_dict(torch.load(filename, map_location=device))


def training() -> None:
    torch.manual_seed(2531)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 128
    n_epochs = 1
    lr = 1e-3
    lr_step_interval = 30
    pre_trained = False

    log_interval = 3
    saving_interval = 10
    visualization_interval = 1

    train_data_base = get_train_data_base_path('train.csv')
    validation_data_base = get_validation_data_base_path('validation.csv')
    test_data_base = get_test_data_base_path('test.csv')

    train_data_set = UnLabeledDataSet(train_data_base)
    validation_data_set = UnLabeledDataSet(validation_data_base)
    test_data_set = UnLabeledDataSet(test_data_base)
    test_labeled_data_set = LabeledDataSet(test_data_base)

    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True, num_workers=1)
    validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False, num_workers=1)
    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False, num_workers=1)
    test_labeled_loader = DataLoader(test_labeled_data_set, batch_size=batch_size, shuffle=False, num_workers=1)

    training_space = create_training_space(model_name='AE2')

    d_in = train_data_set.sample_size
    d_h = 200
    d_l = 2
    dropout_rate = 0.1

    model = AE2(d_in=d_in, d_h=d_h, d_l=d_l, dropout_rate=dropout_rate).to(device)

    if pre_trained:
        model.load_parameters(get_check_point_parameters_path(training_space['check_point']), device=device)

    loss_function = nn.BCELoss(reduction='mean')
    optimizer = optim.Adam(model.parameters(), lr=lr)
    ae_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_step_interval)

    with open(training_space['log'], 'w') as logger:
        logger.write(train_data_base + '\n')
        logger.write(validation_data_base + '\n')
        logger.write(f'batch size: {batch_size}\n')
        logger.write(f'n_epochs: {n_epochs}\n')
        logger.write(f'lr: {lr}\n')
        logger.write(f'lr_step_interval: {lr_step_interval}\n')
        logger.write(f'pre_trained: {pre_trained}\n')
        logger.write(str(model))
        logger.write(f'\n{optimizer}\n')

    loss_history = {'loss': [], 'avg_loss': [], 'valid_loss': []}

    try:
        for i_epoch in range(1, n_epochs + 1):
            epoch_training(i_epoch, model, train_loader, optimizer, device, loss_function, log_interval,
                           training_space,
                           loss_history)
            epoch_validation(model, validation_loader, device, loss_function, training_space, loss_history)

            if i_epoch % saving_interval == 0:
                save_parameters(model, training_space, i_epoch)

            if i_epoch % visualization_interval == 0:
                epoch_visualization(training_space, loss_history, model, i_epoch, test_loader,
                                    test_data_set, device)

            ae_lr_scheduler.step()

        if model.d_l == 2:
            ae_scatter_labeled_latent_space(training_space, model, test_labeled_loader, device)

    except KeyboardInterrupt:
        print('Interrupted')


if __name__ == '__main__':
    matplotlib.rcParams.update({'font.size': 22, 'figure.facecolor': (239 / 255, 246 / 255, 231 / 255),
                                'axes.facecolor': (239 / 255, 246 / 255, 231 / 255),
                                'savefig.facecolor': (239 / 255, 246 / 255, 231 / 255)})

    training()

    pass
