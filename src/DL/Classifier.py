from os.path import join

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import torch
from sklearn.metrics import (classification_report, precision_score, recall_score, f1_score)
from torch import nn, optim
from torch.nn import functional as F
from torch.optim import lr_scheduler
from torch.utils.data import DataLoader

from src.Common.Extraction import LabelExtractor
from src.Common.utils import (get_validation_data_base_path, create_training_space,
                              get_data_base_path, get_check_point_parameters_path,
                              get_file_id, get_train_data_base_path, get_test_data_base_path)
from src.DB.Datasets import LabeledDataSet
from src.DL.utils import loss_visualization, save_parameters


class Classifier(nn.Module):
    def __init__(self, d_in: int, d_h: int, n_classes: int):
        super(Classifier, self).__init__()

        self.cl_fc1 = nn.Linear(d_in, d_h)
        self.cl_fc2 = nn.Linear(d_h, n_classes)

    def forward(self, x):
        x = F.relu(self.cl_fc1(x))

        return self.cl_fc2(x)

    def save_parameters(self, filename: str):
        torch.save(self.state_dict(), filename)

    def load_parameters(self, filename: str, device: torch.device):
        self.load_state_dict(torch.load(filename, map_location=device))


def epoch_training(i_epoch: int, classifier: nn.Module, train_loader: DataLoader, optimizer, device: torch.device,
                   log_interval: int, training_space: dict, loss_history: dict, metrics_history: dict,
                   loss_func, printable: bool = True) -> None:
    classifier.train()
    train_loss = 0
    predicted = list()
    labels = list()
    n_batches = 0

    for batch_idx, (batch_data, batch_labels) in enumerate(train_loader, start=1):
        n_batches = batch_idx
        batch_data = batch_data.to(device)
        batch_labels = batch_labels.to(device)

        optimizer.zero_grad()

        recon_batch = classifier(batch_data)

        loss = loss_func(recon_batch, batch_labels.long())
        loss.backward()
        train_loss += loss.item()

        optimizer.step()

        predicted.append(torch.max(recon_batch, dim=1)[1])
        labels.append(batch_labels)

        if batch_idx % log_interval == 0:
            i_epoch_loss = loss.item()
            i_epoch_info = f'Train Epoch: {i_epoch} [{batch_idx * len(batch_data)}/{len(train_loader.dataset)} ' \
                           f'{100. * batch_idx / len(train_loader):.0f}%)]\tLoss: {i_epoch_loss:.6f}'

            if printable:
                print(i_epoch_info)

            loss_history['loss'].append(i_epoch_loss)

            with open(training_space['log'], 'a') as logger:
                logger.write(i_epoch_info + '\n')

    predicted = torch.cat(predicted).detach().cpu().numpy()
    labels = torch.cat(labels).cpu().detach().numpy()

    avg_epoch_loss = train_loss / n_batches
    precision = precision_score(labels, predicted, average='micro')
    recall = recall_score(labels, predicted, average='micro')
    f1 = f1_score(labels, predicted, average='micro')

    epoch_info = f'====> Epoch: {i_epoch} Average loss: {avg_epoch_loss:.4f} Precision: {precision:.4f} ' \
                 f'Recall: {recall:.4f} F1: {f1 :.4f}'

    # metrics_info = classification_report(labels, predicted, zero_division=0)

    if printable:
        print(epoch_info)
        # print(metrics_info)

    loss_history['avg_loss'].append(avg_epoch_loss)
    metrics_history['f1'].append(f1)

    with open(training_space['log'], 'a') as logger:
        logger.write(epoch_info + '\n')
        # logger.write(metrics_info + '\n')


def epoch_validation(classifier: nn.Module, validation_loader: DataLoader, device: torch.device,
                     training_space: dict, loss_history: dict, metrics_history: dict, loss_func,
                     printable: bool = True) -> None:
    classifier.eval()
    validation_loss = 0
    predicted = list()
    labels = list()
    n_batches = 0

    with torch.no_grad():
        for i, (batch_data, batch_labels) in enumerate(validation_loader, start=1):
            n_batches = i
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)

            recon_batch = classifier(batch_data)

            validation_loss += loss_func(recon_batch, batch_labels.long())

            predicted.append(torch.max(recon_batch, dim=1)[1])
            labels.append(batch_labels)

    validation_loss /= n_batches
    predicted = torch.cat(predicted).detach().cpu().numpy()
    labels = torch.cat(labels).detach().cpu().numpy()

    validation_info = f'====> Validation loss: {validation_loss:.4f}\n'
    validation_info += str(classification_report(labels, predicted, target_names=LabelExtractor.labels,
                                                 zero_division=0))

    precision = precision_score(labels, predicted, average='micro')
    recall = recall_score(labels, predicted, average='micro')
    f1 = f1_score(labels, predicted, average='micro')

    if printable:
        print(validation_info)

    loss_history['valid_loss'].append(validation_loss)
    metrics_history['valid_f1'].append(f1)
    metrics_history['valid_precision'].append(precision)
    metrics_history['valid_recall'].append(recall)

    with open(training_space['log'], 'a') as logger:
        logger.write(validation_info + '\n')


def metrics_visualization(training_space: dict, metrics_history: dict, i_epoch: int) -> None:
    n_epoch = len(metrics_history['f1'])

    fig, ax = plt.subplots(figsize=(19.2, 10.8))

    ax.plot(metrics_history['f1'], label='Training F1', color='royalblue')
    ax.plot(metrics_history['valid_f1'], label='Validation F1', color='springgreen')
    # ax.plot(metrics_history['valid_precision'], label='Validation precision', color='tomato')
    # ax.plot(metrics_history['valid_recall'], label='Validation recall', color='gold')

    x_ticks = np.arange(n_epoch)
    x_labels = np.array([str(ep + 1) for ep in x_ticks])
    if n_epoch > 15:
        mask = np.linspace(start=0, stop=n_epoch - 1, num=15, dtype=np.int32)
        x_ticks = x_ticks[mask]
        x_labels = x_labels[mask]

    ax.set_title(f'Metrics changing')
    ax.set_ylabel('Value')
    ax.set_xlabel('Epoch')
    ax.set_xticks(x_ticks)
    ax.set_xticklabels(x_labels)
    ax.set_ybound(lower=0)
    ax.legend()

    fig.tight_layout()

    plt.savefig(join(training_space['performance'], f'{training_space["mark"]}__epoch_{i_epoch}.png'), dpi='figure',
                format='png', bbox_inches='tight')
    plt.clf()
    plt.close('all')


def epoch_visualization(training_space: dict, loss_history: dict, metrics_history: dict, i_epoch: int) -> None:
    loss_visualization(training_space, loss_history, i_epoch)
    metrics_visualization(training_space, metrics_history, i_epoch)


def probabilities_distribution_visualization(correct: dict, incorrect: dict, incorrect_target: dict,
                                             training_space: dict) -> None:
    def get_ratio(dist: dict) -> np.ndarray:
        n = sum(dist.values())
        ratio = np.array(list(dist.values()))

        return ratio / n

    thresholds = [f'{lbl + 0.1:.1f}-{lbl:.1f}' for lbl in correct.keys()]
    x_ticks = np.arange(0, len(thresholds))
    y_ticks = np.arange(0, 1, 0.1)
    y_labels = [f'{int(y_tick * 100)}%' for y_tick in y_ticks]
    correct_ratio = get_ratio(correct)
    incorrect_ratio = get_ratio(incorrect)
    incorrect_target_ratio = get_ratio(incorrect_target)
    width = 0.2

    fig, ax = plt.subplots(figsize=(19.2, 10.8))

    ax.bar(x_ticks, correct_ratio, width, color='lightseagreen', label='True Positive')
    ax.bar(x_ticks + width, incorrect_ratio, width, color='lightcoral', label='False Positive + False Negative')
    ax.bar(x_ticks + width * 2, incorrect_target_ratio, width, color='mediumslateblue',
           label='Target among wrong cases')

    ax.set_title(f"Classifier prediction ratio")
    ax.set_ylabel('Percent')
    ax.set_xlabel('Thresholds')
    ax.set_xticks(x_ticks + ((width * 3) / 3))
    ax.set_xticklabels(thresholds)
    ax.set_yticks(y_ticks)
    ax.set_yticklabels(y_labels)
    ax.set_ybound(lower=0)
    ax.legend()

    fig.tight_layout()

    file_id = get_file_id(training_space['performance'])

    plt.savefig(join(training_space['performance'], f'{training_space["mark"]}__{file_id}.png'), dpi='figure',
                format='png', bbox_inches='tight')
    plt.clf()
    plt.close('all')


def get_probabilities_distribution(predicted: np.ndarray, target: np.ndarray, probabilities: np.ndarray,
                                   training_space: dict, visualise: bool = True) -> tuple:
    mask = predicted == target
    thresholds = np.arange(0.0, 1.0, 0.1)[::-1]
    initial = np.zeros(len(thresholds))

    correct = dict(zip(thresholds, initial))
    incorrect = dict(zip(thresholds, initial))
    incorrect_target = dict(zip(thresholds, initial))

    for i in range(len(mask)):
        if mask[i]:
            for k in correct.keys():
                if probabilities[i, int(target[i])] >= k:
                    correct[k] += 1
                    break
        else:
            for k in incorrect.keys():
                if probabilities[i, int(predicted[i])] >= k:
                    incorrect[k] += 1
                    break

            for k in incorrect_target.keys():
                if probabilities[i, int(target[i])] >= k:
                    incorrect_target[k] += 1
                    break

    if visualise:
        probabilities_distribution_visualization(correct, incorrect, incorrect_target, training_space)

    return correct, incorrect, incorrect_target


def get_error_probabilities(predicted: np.ndarray, target: np.ndarray, probabilities: np.ndarray,
                            training_space: dict) -> str:
    def prepare_probability(probability: np.ndarray) -> str:
        return ' '.join([f'{p:.6f}' for p in probability])

    def prepare_dist(dist: dict) -> str:
        return '\n'.join([f'{it[0]:0.1f}: {it[1]}' for it in dist.items()])

    mask = predicted == target
    info = '\n\nErrors probabilities:\n'

    for case, prob in enumerate(probabilities):
        if not mask[case]:
            info += f'{prepare_probability(prob)}  :  {target[case]}\n'

    correct, incorrect, incorrect_target = get_probabilities_distribution(predicted, target, probabilities,
                                                                          training_space, visualise=True)

    info += f'\n\nCorrect distribution:\n{prepare_dist(correct)}'
    info += f'\n\nIncorrect distribution:\n{prepare_dist(incorrect)}'
    info += f'\n\nIncorrect target distribution:\n{prepare_dist(incorrect_target)}'

    return info


def create_confusion_matrix(predicted: np.ndarray, target: np.ndarray, printable: bool = True) -> np.ndarray:
    n = len(LabelExtractor.labels)
    confusion_matrix = np.zeros((n, n))

    for predicted_label, target_label in zip(predicted.astype(np.int32), target.astype(np.int32)):
        confusion_matrix[predicted_label, target_label] += 1

    if printable:
        print(confusion_matrix)

    return confusion_matrix


def testing(classifier: nn.Module, test_loader: DataLoader, device: torch.device,
            training_space: dict, loss_func, printable: bool = True) -> None:
    classifier.eval()
    test_loss = 0
    predicted = list()
    target = list()
    probabilities = list()
    n_batches = 0

    from datetime import datetime

    with torch.no_grad():
        for i, (batch_data, batch_target) in enumerate(test_loader, start=1):
            n_batches = i
            batch_data = batch_data.to(device)
            batch_target = batch_target.to(device)

            s = datetime.now().microsecond
            recon_batch = classifier(batch_data)
            print(datetime.now().microsecond - s)

            predicted.append(torch.max(recon_batch, dim=1)[1])
            target.append(batch_target)
            probabilities.append(F.softmax(recon_batch, dim=1))

            test_loss += loss_func(recon_batch, batch_target.long())

    test_loss /= n_batches
    predicted = torch.cat(predicted).detach().cpu().numpy()
    target = torch.cat(target).detach().cpu().numpy()
    probabilities = torch.cat(probabilities).detach().cpu().numpy()

    validation_info = f'====> Testing loss: {test_loss:.4f}\n'
    validation_info += str(classification_report(target, predicted, target_names=LabelExtractor.labels,
                                                 zero_division=0))
    validation_info += get_error_probabilities(predicted, target, probabilities, training_space)

    if printable:
        print(validation_info)

    confusion_matrix = create_confusion_matrix(predicted, target)
    validation_info += f'\nConfusion matrix:\n{str(confusion_matrix)}'

    with open(training_space['log'], 'a') as logger:
        logger.write(validation_info + '\n')


def training() -> None:
    torch.manual_seed(2531)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    batch_size = 512
    n_epochs = 40
    lr = 1e-3
    lr_step_interval = 30
    log_interval = 3
    saving_interval = 5
    visualization_interval = 5
    pre_trained = False

    train_data_base = get_train_data_base_path('train.csv')
    validation_data_base = get_validation_data_base_path('validation.csv')
    test_data_base = get_test_data_base_path('test.csv')
    # remains_data_base = get_data_base_path('remains.csv')
    # artificial_data_base = get_data_base_path('artificial_database.csv')

    train_data_set = LabeledDataSet(train_data_base)
    validation_data_set = LabeledDataSet(validation_data_base)
    test_data_set = LabeledDataSet(test_data_base)
    # remains_data_set = LabeledDataSet(remains_data_base)
    # artificial_data_set = LabeledDataSet(artificial_data_base)

    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=True,
                              num_workers=1, pin_memory=True)
    validation_loader = DataLoader(validation_data_set, batch_size=batch_size, shuffle=False,
                                   num_workers=1, pin_memory=True)
    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=False,
                             num_workers=1, pin_memory=True)
    # remains_loader = DataLoader(remains_data_set, batch_size=batch_size, shuffle=False,
    #                             num_workers=1, pin_memory=True)
    # artificial_loader = DataLoader(artificial_data_set, batch_size=batch_size, shuffle=False,
    #                                num_workers=1, pin_memory=True)

    training_space = create_training_space(model_name='Classifier')

    d_in = train_data_set.sample_size
    d_h = 8
    n_classes = train_data_set.n_classes

    classifier = Classifier(d_in=d_in, d_h=d_h, n_classes=n_classes).to(device)

    if pre_trained:
        classifier.load_parameters(get_check_point_parameters_path(training_space['check_point']), device=device)

    optimizer = optim.Adam(classifier.parameters(), lr=lr)
    classifier_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_step_interval)

    loss_func = nn.CrossEntropyLoss()

    with open(training_space['log'], 'w') as logger:
        logger.write(train_data_base + '\n')
        logger.write(validation_data_base + '\n')
        logger.write(f'batch size: {batch_size}\n')
        logger.write(f'n_epochs: {n_epochs}\n')
        logger.write(f'lr: {lr}\n')
        logger.write(f'lr_step_interval: {lr_step_interval}\n')
        logger.write(f'pre_trained: {pre_trained}\n')
        logger.write(str(classifier))
        logger.write(f'{str(optimizer)}\n')
        logger.write(f'{str(loss_func)}\n')

    loss_history = {'loss': [], 'avg_loss': [], 'valid_loss': []}
    metrics_history = {'f1': [], 'valid_f1': [], 'valid_precision': [], 'valid_recall': []}

    try:
        for i_epoch in range(1, n_epochs + 1):
            epoch_training(i_epoch, classifier, train_loader, optimizer, device, log_interval, training_space,
                           loss_history, metrics_history, loss_func)
            epoch_validation(classifier, validation_loader, device, training_space, loss_history, metrics_history,
                             loss_func)

            if i_epoch % saving_interval == 0:
                save_parameters(classifier, training_space, i_epoch)

            if i_epoch % visualization_interval == 0:
                epoch_visualization(training_space, loss_history, metrics_history, i_epoch)

            classifier_lr_scheduler.step()

        testing(classifier, test_loader, device, training_space, loss_func)
        # testing(classifier, remains_loader, device, training_space, loss_func)
        # testing(classifier, artificial_loader, device, training_space, loss_func)

    except KeyboardInterrupt:
        print('Interrupted')


if __name__ == '__main__':
    matplotlib.rcParams.update({'font.size': 22})

    training()

    pass
