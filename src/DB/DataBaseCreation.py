import json
from multiprocessing import Manager, Process
from os.path import join

import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from src.Common import config
from src.Common.Extraction import FeatureExtractor, LabelExtractor
from src.Common.SharedMemory import SharedReportPaths, SharedReportsFeatures
from src.Common.utils import (get_report_paths, get_debug_report_paths, get_file_id, save_obj, get_data_base,
                              get_data_base_path)


def parsing_reports(report_paths: SharedReportPaths, reports_features: SharedReportsFeatures) -> None:
    extractor = FeatureExtractor()

    while report_paths:
        report_path = report_paths.get_report_path()
        print(report_path)

        with open(report_path, 'r') as js_f:
            js_data = json.load(js_f)

        extracted_features = extractor.get_features(js_data)
        reports_features.add_report_feature(extracted_features)


def create_data_base(paths: tuple = (config.reports_path, config.info_lack_reports_path),
                     debug: bool = False, process_count: int = 12) -> None:
    if debug:
        report_paths = get_debug_report_paths()
    else:
        report_paths = get_report_paths(paths)

    manager = Manager()
    report_paths = SharedReportPaths(report_paths, manager)
    reports_features = SharedReportsFeatures(manager)

    ps = [Process(target=parsing_reports, args=(report_paths, reports_features)) for _ in range(process_count)]

    for p in ps:
        p.start()

    for p in ps:
        p.join()

    reports_features = reports_features.get_reports_features()

    data_base = pd.DataFrame(reports_features)

    data_base_info(data_base)

    data_base_id = get_file_id(config.data_base_path)
    data_base.to_csv(path_or_buf=join(config.data_base_path, f'database_{data_base_id:_>3}.csv'), index=False)


def scale_data_base(data_base_file: str, scale, scale_type: str) -> None:
    data_base_id = get_file_id(config.data_base_path)
    data_base_name = data_base_file.split('.')[0]
    data_base = get_data_base(data_base_file)

    data_base_labels = data_base[data_base.columns[:2]]
    data_base = data_base.drop(data_base.columns[:2], axis=1)

    data_base_std = pd.DataFrame(scale.fit_transform(data_base), columns=data_base.columns)

    save_obj(scale, filename=f'{scale_type}_scale_{data_base_name}_{data_base_id:_>3}')

    data_base = pd.concat([data_base_labels, data_base_std], axis=1)

    data_base.to_csv(path_or_buf=join(config.data_base_path, f'{data_base_name}_{scale_type}_{data_base_id:_>3}.csv'),
                     index=False)


def normalize_data_base(data_base_file: str) -> None:
    scale_data_base(data_base_file, MinMaxScaler(), 'norm')


def standardize_data_base(data_base_file: str) -> None:
    scale_data_base(data_base_file, StandardScaler(), 'std')


def split_data_base(filename: str, train: int = 60, valid: int = 20) -> None:
    data_base = get_data_base(filename, labels=True)
    drop_duplicates(data_base)
    data_base = data_base.sample(frac=1).reset_index(drop=True)

    data_base_size = data_base.shape[0]

    train = (data_base_size * train) // 100
    valid = (data_base_size * valid) // 100

    train_data_base = data_base[:train]
    valid_data_base = data_base[train:train + valid]
    test_data_base = data_base[train + valid:]

    print('Train')
    data_base_info(train_data_base)
    print('Validation')
    data_base_info(valid_data_base)
    print('Test')
    data_base_info(test_data_base)

    train_data_base.to_csv(path_or_buf=join(config.train_data_base_path, f'train_{filename}'), index=False)
    valid_data_base.to_csv(path_or_buf=join(config.validation_data_base_path, f'validation_{filename}'), index=False)
    test_data_base.to_csv(path_or_buf=join(config.test_data_base_path, f'test_{filename}'), index=False)


def unite_data_bases(data_base_file_1: str, data_base_file_2: str) -> None:
    data_base_1 = get_data_base(get_data_base_path(data_base_file_1))
    data_base_2 = get_data_base(get_data_base_path(data_base_file_2))

    united = data_base_1.append(data_base_2)
    drop_duplicates(united)

    united.to_csv(
        path_or_buf=join(config.data_base_path, f'UNITED_{data_base_file_1.split(".")[0]}_AND_{data_base_file_2}'),
        index=False)


def label_data_base(data_base_file: str, heuristics: bool):
    label_extractor = LabelExtractor()

    labeled_data_base, heuristic, unknown, unlabeled, unlabeled_hashes = label_extractor.get_labeled_data_base(
        data_base_file=get_data_base_path(data_base_file),
        heuristics=heuristics)

    data_base_id = get_file_id(config.data_base_path)
    data_base_name = data_base_file.split('.')[0]

    if len(labeled_data_base) > 0:
        drop_duplicates(labeled_data_base)
        labeled_data_base.to_csv(
            path_or_buf=join(config.data_base_path,
                             f'{data_base_name}__labeled_{data_base_id:_>3}_heur{heuristics}.csv'),
            index=False)

        print('labeled')
        data_base_info(labeled_data_base, printable=True)

    if not heuristics and len(heuristic) > 0:
        drop_duplicates(heuristic)
        heuristic.to_csv(
            path_or_buf=join(config.data_base_path,
                             f'{data_base_name}__heuristic_{data_base_id:_>3}_heur{heuristics}.csv'),
            index=False)

        print('heuristic')
        data_base_info(heuristic, printable=True)

    if len(unknown) > 0:
        drop_duplicates(unknown)
        unknown.to_csv(
            path_or_buf=join(config.data_base_path,
                             f'{data_base_name}__unknown_{data_base_id:_>3}_heur{heuristics}.csv'),
            index=False)

        print('unknown')
        data_base_info(unknown, printable=True)

    if len(unlabeled) > 0:
        drop_duplicates(unlabeled)
        unlabeled.to_csv(
            path_or_buf=join(config.data_base_path,
                             f'{data_base_name}__unlabeled_{data_base_id:_>3}_heur{heuristics}.csv'),
            index=False)

        print('unlabeled')
        data_base_info(unlabeled, printable=True)

    if len(unlabeled_hashes) > 0:
        param_id = get_file_id(config.parameters_path)
        save_obj(unlabeled_hashes, filename=f'unlabeled_hashes_{data_base_name}_{param_id:_>3}')


def data_base_info(data_base: pd.DataFrame, printable: bool = True) -> str:
    info = f'{"*" * 20}\n'
    freq = data_base[data_base.columns[1]].value_counts()

    info += f'Shape: {data_base.shape}\n'
    info += f'{"*" * 20}\n'
    info += str(freq) + '\n'
    info += f'{"*" * 20}\n'
    info += f'Mean: {freq.mean()}\n'
    info += f'Median: {freq.median()}\n'
    info += f'{"*" * 20}\n'

    if printable:
        print(info)

    return info


def balance_data_base(data_base_file: str) -> None:
    data_base = get_data_base(data_base_file, labels=True)
    drop_duplicates(data_base)

    labels = data_base[data_base.columns[1]]
    threshold = int(labels.value_counts().mean())
    classes = list()

    classes_names = labels.unique()

    for c in classes_names:
        c_data = data_base[labels == c]
        classes.append(c_data.iloc[:threshold])

    for c_data in classes:
        data_base.drop(c_data.index, inplace=True)

    balanced_data_base = pd.concat(classes, axis=0, ignore_index=True)

    print('Balanced')
    data_base_info(balanced_data_base)
    print('Remains')
    data_base_info(data_base)

    data_base.to_csv(path_or_buf=join(config.data_base_path, f'balanced_remains_{data_base_file}'), index=False)
    balanced_data_base.to_csv(path_or_buf=join(config.data_base_path, f'balanced_{data_base_file}'), index=False)


def drop_duplicates(data_base: pd.DataFrame) -> None:
    data_base.drop_duplicates(data_base.columns[0], inplace=True)


def experience():
    data_base = pd.read_csv(filepath_or_buffer=join(config.data_base_path, 'debug_database.csv'))

    print(data_base.shape)

    labels_data_base = data_base[data_base.columns[:2]]
    data_base = data_base.drop(data_base.columns[:2], axis=1)

    std_scale = StandardScaler()

    data_base_std = pd.DataFrame(std_scale.fit_transform(data_base), columns=data_base.columns)

    pd.concat([labels_data_base, data_base_std], axis=1)


if __name__ == '__main__':
    create_data_base(paths=(config.reports_path, config.info_lack_reports_path),
                     debug=False)

    # experience()

    # split_data_base(
    #     'balanced_database_1.csv',
    #     train=80, valid=10)

    # unite_data_bases('database_1.csv',
    #                  'database_2.csv')

    # normalize_data_base('database_3.csv')

    # label_data_base('database_3.csv',
    #                 heuristics=False)

    # _data_base = get_data_base(
    #     'UNITED_database_1.csv',
    #     labels=True)

    # data_base_info(_data_base, printable=True)

    # drop_duplicates(_data_base)
    # data_base_info(_data_base, printable=True)

    # balance_data_base(
    #     'UNITED_database_1.csv')

    pass
