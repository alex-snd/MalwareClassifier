import json
from collections import OrderedDict
from multiprocessing import Manager, Process, Lock, Value
from os import listdir, mkdir
from os.path import join, basename, exists

from src.Common import config
from src.Common.Extraction import LabelExtractor
from src.Common.utils import get_debug_report_paths, get_report_paths, move_to_dirty


def deepening(_data: dict, _json_data, _way: tuple,
              _lock_data: Lock, _flag):
    if len(_way) > 0:
        _w = _way[0]

        if isinstance(_w, tuple):
            try:
                deepening(_data, _json_data, _way[1:], _lock_data, _flag)
            except KeyError:
                pass
            except TypeError:
                pass

        elif isinstance(_w, list):
            for _js_dt in _json_data:
                deepening(_data, _js_dt, _way[1:], _lock_data, _flag)

        elif isinstance(_w, dict):
            for _js_dt in _json_data.keys():
                deepening(_data, _js_dt, tuple(), _lock_data, _flag)

        else:
            deepening(_data, _json_data[_w], _way[1:], _lock_data, _flag)

    else:
        if isinstance(_json_data, list):
            for _js_dt in _json_data:
                deepening(_data, _js_dt, tuple(), _lock_data, _flag)
            print('[] was added to way\n')
            return

        elif isinstance(_json_data, dict):
            for _js_dt in _json_data.keys():
                deepening(_data, _js_dt, tuple(), _lock_data, _flag)
            print('{} was added to way\n')
            return

        if _json_data is None or _json_data == '':
            _json_data = 'NONE'

        with _lock_data:
            _data.setdefault(_json_data, 0)
            _data[_json_data] += 1

            if _json_data != 'NONE':
                _flag['contains_data'] = True


def is_not_target(target: str, json_data: dict, _lock_data: Lock, flag: dict,
                  target_info: dict, heuristics: bool) -> bool:
    dt = dict()
    _way = ('target', 'file', 'sha256')

    deepening(dt, json_data, _way, _lock_data, flag)

    hash_id = dt.popitem()[0]

    if hash_id in target_info and target_info[hash_id] != 'NONE':
        _label = target_info[hash_id].replace('HEUR:', '') if heuristics else target_info[hash_id]

        if 'HEUR:' not in _label and 'Trojan' in _label:
            return False if 'Trojan' == target else True

        if 'HEUR:' not in _label and 'Worm' in _label:
            return False if 'Worm' == target else True

        return False if _label == target else True

    else:
        return True


def report_analysis(_report_paths: list,
                    _total: Value,
                    _data: dict,
                    _contained_data_files: list,
                    _lock_report_paths: Lock,
                    _lock_total: Lock,
                    _lock_data: Lock,
                    _lock_contained_data_files: Lock,
                    _way: tuple,
                    target_class: str = None,
                    target_info: dict = None,
                    heuristics: bool = False,
                    by_js_file: bool = False):
    flag = {'contains_data': False}

    while True:

        with _lock_report_paths:
            if len(_report_paths) == 0:
                return

            json_filename = _report_paths[0]
            _report_paths.remove(json_filename)

        try:
            with open(json_filename) as j_f:
                json_data = json.load(j_f)

            if target_class and is_not_target(target_class, json_data, _lock_data, flag, target_info, heuristics):
                with _lock_total:
                    _total.value -= 1
                continue

            if by_js_file:
                dt = dict()

                deepening(dt, json_data, _way, _lock_data, flag)

                for k in dt.keys():
                    _data.setdefault(k, 0)
                    _data[k] += 1
            else:
                deepening(_data, json_data, _way, _lock_data, flag)

            if flag['contains_data']:
                with _lock_contained_data_files:
                    _contained_data_files.append(json_filename)

                flag['contains_data'] = False

        except json.decoder.JSONDecodeError:
            move_to_dirty(json_filename)

            with _lock_total:
                _total.value -= 1

        except KeyError as e:
            print(f'Некоректный путь.\n{e}')
            return


def save_analysis(total: int, data: dict, contained_data_files: list, way: tuple, debug: bool, by_js_file: bool):
    if len(data) == 0:
        print(f'Way {way} does not contains any info.')
        return

    data = OrderedDict(sorted(data.items(), key=lambda d: -d[1]))
    data_keys = data.keys()
    max_keys_len = max([len(str(k)) for k in data_keys])

    if max_keys_len < len('UnicodeEncodeError'):
        max_keys_len = len('UnicodeEncodeError')

    if not exists(config.analyse_path):
        mkdir(config.analyse_path)

    file_id = len(listdir(config.analyse_path)) - 1
    filename = str(way)
    with open(join(config.analyse_path, f'{file_id}_{filename}.txt'), 'w') as f:
        f.write(
            f'Debug: {debug}  ||  By json file: {by_js_file}\nDict size: {len(data)}\nTotal: {int(total)}\n\n')

        for k, v in data.items():
            try:
                f.write(f"{k:<{max_keys_len}} {v}\n")
            except UnicodeEncodeError:
                f.write(f"{'UnicodeEncodeError':<{max_keys_len}} {v}\n")

        if len(contained_data_files) != total:
            for file in contained_data_files[:10]:
                f.write(f'\n{file}')

    machine_path = join(config.analyse_path, '[MACHINE]')
    if not exists(machine_path):
        mkdir(config.analyse_path)

    with open(join(machine_path, f'{file_id}_{filename}[MACHINE].txt'), 'w') as f:
        f.write(
            f'Debug: {debug}  ||  By json file: {by_js_file}\nDict size: {len(data)}\nTotal: {int(total)}\n\n')

        for k in data.keys():
            try:
                f.write(f"{k}\n")
            except UnicodeEncodeError:
                f.write(f"{'UnicodeEncodeError':<{max_keys_len}} {v}\n")
                print('UnicodeEncodeError\n')


def analysis(reports_path: tuple = (config.reports_path,), way: tuple = ('info',), debug: bool = False,
             process_count: int = 12, target_class: str = None, target_info: dict = None, heuristics: bool = False,
             by_js_file: bool = False) -> None:
    manager = Manager()

    if debug:
        report_paths = manager.list(get_debug_report_paths())
    else:
        report_paths = manager.list(get_report_paths(reports_path))

    total = Value('d', len(report_paths))
    data = manager.dict()
    contained_data_files = manager.list()

    lock_report_paths = Lock()
    lock_total = Lock()
    lock_data = Lock()
    lock_contained_data_files = Lock()

    ps = [Process(target=report_analysis, args=(report_paths, total, data, contained_data_files,
                                                lock_report_paths,
                                                lock_total,
                                                lock_data,
                                                lock_contained_data_files,
                                                way,
                                                target_class, target_info, heuristics,
                                                by_js_file)) for _ in range(process_count)]
    for p in ps:
        p.start()

    for p in ps:
        p.join()

    save_analysis(total.value, data, contained_data_files, way, debug, by_js_file)


# **********************************************************************************************************************

def flags_splitting():
    filenames = listdir(config.analyse_path)

    for file in filenames:
        with open(join(config.analyse_path, file), 'r') as f:
            head = []
            body = []

            for _ in range(4):
                head.append(f.readline())

            line = f.readline()
            while line != '\n':
                line = line.split()
                body.append((line[0].split('|'), int(line[-1])))
                line = f.readline()

            ending = f.read()

        data = dict()

        for dt in body:
            for dt_p in dt[0]:
                data.setdefault(dt_p, 0)
                data[dt_p] += dt[1]

        data = OrderedDict(sorted(data.items(), key=lambda d: -d[1]))
        data_keys = data.keys()
        max_keys_len = max([len(str(k)) for k in data_keys])

        with open(join(config.analyse_path, file), 'w') as f:
            f.write(f'{head[0]}Dict size: {len(data)}\n{head[2]}\n')

            for k, v in data.items():
                try:
                    f.write(f"{k:<{max_keys_len}} {v}\n")
                except UnicodeEncodeError:
                    f.write(f"{'UnicodeEncodeError':<{max_keys_len}} {v}\n")

            f.write(f'\n{ending}')

        with open(join(config.analyse_path, f'{file[:-4]}[MACHINE].txt'), 'w') as f:
            f.write(f'{head[0]}Dict size: {len(data)}\n{head[2]}\n')

            try:
                for k in data.keys():
                    f.write(f"{k}\n")
            except UnicodeEncodeError:
                f.write(f"{'UnicodeEncodeError':<{max_keys_len}} {v}\n")
                print('UnicodeEncodeError\n')


def prepare_machine_files():
    filenames = [join(config.analyse_path, file) for file in listdir(config.analyse_path)]

    for file in filenames:
        with open(file) as f:
            for _ in range(4):
                f.readline()

            body = list()

            for line in f:
                body.append(line[:-1])

        print(f'SIZE: {len(body)}')

        with open(file, 'w') as f:
            for line in body[:-1]:
                f.write(f"r'{line}', ")
            f.write(f"r'{body[-1]}'")


def prepare_machine_dll_files():
    filenames = [join(config.analyse_path, file) for file in listdir(config.analyse_path)]

    for file in filenames:
        with open(file) as f:
            for _ in range(4):
                f.readline()

            body = f.read().splitlines()

        prepared_body = list()

        for i in range(len(body)):
            dll_name = body[i].upper()

            if dll_name.endswith('.DLL'):
                dll_name = dll_name[:-4]
            if dll_name.startswith('C:\\WINDOWS\\SYSTEM32\\'):
                dll_name = dll_name[20:]
            elif dll_name.startswith('C:\\WINDOWS\\MICROSOFT.NET\\'):
                dll_name = basename(dll_name)
            elif dll_name.startswith('C:\\WINDOWS\\ASSEMBLY\\'):
                dll_name = basename(dll_name)
            elif dll_name.startswith('C:\\PROGRAM FILES\\COMMON FILES\\SYSTEM\\'):
                dll_name = basename(dll_name)
            elif dll_name.startswith('C:\\WINDOWS\\WINSXS\\'):
                dll_name = basename(dll_name)
            elif dll_name.startswith('C:\\PROGRAM FILES\\INTERNET EXPLORER\\'):
                dll_name = basename(dll_name)
            elif dll_name.startswith('C:\\WINDOWS\\SYSWOW64\\'):
                dll_name = basename(dll_name)

            if dll_name.startswith('C:\\USERS\\'):
                continue
            elif dll_name.startswith('C:\\PROGRAM FILES\\WINRAR\\'):
                continue
            elif dll_name.startswith('SPOOL\\DRIVERS\\'):
                continue
            elif dll_name.startswith('C:\\PROGRAM FILES\\MOZILLA FIREFOX\\'):
                continue
            elif dll_name.startswith('RESOURCES\\'):
                continue

            if dll_name not in prepared_body:
                prepared_body.append(dll_name)

        print(f'SIZE: {len(prepared_body)}')

        with open(file, 'w') as f:
            f.write('\n\n\n\n')
            for line in prepared_body[:-1]:
                f.write(f"{line}\n")
            f.write(f"{prepared_body[-1]}")


def prepare_labels_files():
    filenames = listdir(config.analyse_path)

    for file in filenames:
        with open(join(config.analyse_path, file), 'r') as f:
            head = []
            body = []

            for _ in range(4):
                head.append(f.readline())

            line = f.readline()
            while line != '\n':
                line = line.split()
                body.append((line[0].split('.')[0].replace('HEUR:', ''), int(line[-1])))
                line = f.readline()

            ending = f.read()

        data = dict()

        for dt in body:
            data.setdefault(dt[0], 0)
            data[dt[0]] += dt[1]

        data = OrderedDict(sorted(data.items(), key=lambda d: -d[1]))
        data_keys = data.keys()
        max_keys_len = max([len(str(k)) for k in data_keys])

        with open(join(config.analyse_path, file), 'w') as f:
            f.write(f'{head[0]}Dict size: {len(data)}\n{head[2]}\n')

            for k, v in data.items():
                try:
                    f.write(f"{k:<{max_keys_len}} {v}\n")
                except UnicodeEncodeError:
                    f.write(f"{'UnicodeEncodeError':<{max_keys_len}} {v}\n")

            f.write(f'\n{ending}')

        with open(join(config.analyse_path, f'{file[:-4]}[MACHINE].txt'), 'w') as f:
            f.write(f'{head[0]}Dict size: {len(data)}\n{head[2]}\n')

            try:
                for k in data.keys():
                    f.write(f"{k}\n")
            except UnicodeEncodeError:
                f.write(f"{'UnicodeEncodeError':<{max_keys_len}} {v}\n")
                print('UnicodeEncodeError\n')


# **********************************************************************************************************************

if __name__ == '__main__':
    ways = [('static', (), 'pe_imports', [], 'imports', [], 'name'),
            ('behavior', (), 'summary', (), 'dll_loaded', []),
            ('behavior', (), 'summary', (), 'regkey_read', []),
            ('behavior', (), 'summary', (), 'regkey_opened', []),
            ('behavior', (), 'summary', (), 'file_written', []),
            ('behavior', (), 'summary', (), 'regkey_written', []),
            ('behavior', (), 'summary', (), 'regkey_deleted', []),
            ((), 'dropped', [], 'yara', [], 'name'), ]

    label_extractor = LabelExtractor()

    print('Targets are loaded')

    for _reports_path in [config.reports_path, config.info_lack_reports_path]:
        for w in ways:
            analysis(reports_path=(_reports_path,),
                     way=w,
                     debug=False, process_count=12,
                     target_class=None,
                     target_info=label_extractor.target_info,
                     heuristics=False,
                     by_js_file=True)

    # flags_splitting()
    # prepare_machine_dll_files()
    # prepare_machine_files()
    # prepare_labels_files()

    pass
