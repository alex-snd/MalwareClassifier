import json
from multiprocessing import Manager, Process, Lock
from os.path import join

from src.Common import config
from src.Common.utils import enough_info, move_to_dirty, move_to_info_lack, move_to_duplicates, save_obj
from src.Common.utils import get_debug_report_paths, get_report_paths, get_file_id, get_seen_hashes


def duplication_deepening(_data: dict, _json_data, _way: tuple,
                          _lock_data: Lock, filename: str):
    if len(_way) > 0:
        _w = _way[0]

        if isinstance(_w, tuple):
            try:
                duplication_deepening(_data, _json_data, _way[1:], _lock_data, filename)
            except KeyError:
                pass
            except TypeError:
                pass

        elif isinstance(_w, list):
            for _js_dt in _json_data:
                duplication_deepening(_data, _js_dt, _way[1:], _lock_data, filename)

        elif isinstance(_w, dict):
            for _js_dt in _json_data.keys():
                duplication_deepening(_data, _js_dt, tuple(), _lock_data, filename)

        else:
            duplication_deepening(_data, _json_data[_w], _way[1:], _lock_data, filename)

    else:
        if isinstance(_json_data, list):
            for _js_dt in _json_data:
                duplication_deepening(_data, _js_dt, tuple(), _lock_data, filename)
            print('[] was added to way\n')
            return

        elif isinstance(_json_data, dict):
            for _js_dt in _json_data.keys():
                duplication_deepening(_data, _js_dt, tuple(), _lock_data, filename)
            print('{} was added to way\n')
            return

        if _json_data is None or _json_data == '':
            _json_data = 'NONE'

        with _lock_data:
            _data.setdefault(filename, _json_data)


def report_cleaning(_report_paths: list,
                    _data: dict,
                    _lock_report_paths: Lock,
                    _lock_data: Lock):
    _way = ('target', 'file', 'sha256')

    while True:

        with _lock_report_paths:
            if len(_report_paths) == 0:
                return

            json_filename = _report_paths[0]
            _report_paths.remove(json_filename)

        try:
            with open(json_filename) as j_f:
                json_data = json.load(j_f)

            if not enough_info(json_data):
                move_to_info_lack(json_filename)

            duplication_deepening(_data, json_data, _way, _lock_data, json_filename)

        except json.decoder.JSONDecodeError:
            move_to_dirty(json_filename)

        except KeyError as e:
            print(f'Некоректный путь.\n{e}')
            return


def cleaning(reports_path: tuple = (config.reports_path,), debug: bool = False, process_count: int = 12,
             seen_hashes_file: str = None):
    manager = Manager()

    if debug:
        report_paths = manager.list(get_debug_report_paths())
    else:
        report_paths = manager.list(get_report_paths(paths=reports_path))

    data = manager.dict()

    lock_report_paths = Lock()
    lock_data = Lock()

    ps = [Process(target=report_cleaning, args=(report_paths,
                                                data,
                                                lock_report_paths,
                                                lock_data)) for _ in range(process_count)]
    for p in ps:
        p.start()

    for p in ps:
        p.join()

    if seen_hashes_file:
        hashes = get_seen_hashes(seen_hashes_file)
    else:
        hashes = dict()

    for i_report in data.keys():

        i_hash = data[i_report]

        if i_hash in hashes:
            move_to_duplicates(i_report, hashes[i_hash])
        else:
            hashes.setdefault(i_hash, i_report)

    hash_id = get_file_id(config.parameters_path)
    save_obj(hashes, join(config.parameters_path, f'hashes_{hash_id:_>3}'))


if __name__ == '__main__':
    cleaning(debug=False, process_count=6, seen_hashes_file=None)

    pass
